# Logicimport torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimimport torch.nn.utils.rnn as utilsimport numpy as npimport time# UIimport curses as cursesimport matplotlib.pyplot as pltclass InquiryDataset:    """    Manages the training datasets.    """    contact = np.array([1,    0     , 1, 1, 1, 1, 1, 1, 1, 1])    dataset_search = np.array([1, 1,  0   , 1, 1, 1, 1, 1, 1, 1])    delivery = np.array([1, 1, 1,   0   , 1, 1, 1, 1, 1, 1])    user_interaction_needed = np.array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1])    order = np.array([1, 1, 1, 1,   0  , 1, 1, 1, 1, 1])    welcome = np.array([1,1,1,1,1,0,1,1,1,1])    @staticmethod    def getTrainingDataset() -> np.ndarray:        """        Returns a training dataset [19, 2] that contains inquiries and classifications respectively.        :return: Returns a numpy array [19, 2] with inquiries and classifications        """        return np.array([["Hi, what's your phone number?", InquiryDataset.contact],                         ["Hi, may I get the phone number?", InquiryDataset.contact],                         ["Hi, I want to order this product", (InquiryDataset.order + InquiryDataset.dataset_search), ],                         ["Good morning, I'm interested in this product. What's the price of this one?", InquiryDataset.dataset_search],                         ["Are there any of this product in stock?", InquiryDataset.dataset_search],                         ["What are the ways to deliver the product?", InquiryDataset.delivery],                         ["How long does the delivery take?", InquiryDataset.delivery],                         ["Are any of this product available?", InquiryDataset.dataset_search],                         ["What about the delivery?", InquiryDataset.delivery],                         ["Does it have this characteristic?", InquiryDataset.user_interaction_needed],                         ["We want to cooperate with your company.", InquiryDataset.user_interaction_needed],                         ["We would be glad to work with your company indeed.", InquiryDataset.user_interaction_needed],                         ["Hi, name, is that you?", InquiryDataset.user_interaction_needed],                         ["Is the delivery available to Ukraine?", InquiryDataset.delivery],                         ["Is the delivery available to this country?", InquiryDataset.delivery],                         ["What's the price of the delivery?", InquiryDataset.delivery],                         ["May I pay for the this product when I receive it?", InquiryDataset.delivery],                         ["What is included in the set?", InquiryDataset.dataset_search],                         ["What is included with the product?", InquiryDataset.dataset_search],                         ["Is the delivery for free?", InquiryDataset.delivery],                         ["How long is it going to take to get it to my city?", InquiryDataset.delivery],                         ["Hey, what's the price of the new Nike?", InquiryDataset.dataset_search],                         ["Hi, are any of those shoes available?", InquiryDataset.dataset_search],                         ["Good morning, I'm looking for a new phone for my kid. It shouldn't be expensive. Thank you!", InquiryDataset.dataset_search],                         ["Hey, I'm interested in Adidas leggings? How much do you want for them?", InquiryDataset.dataset_search],                         ["Hey, is the discount still available for the new Puma Cali?", InquiryDataset.dataset_search],                         ["Hi", InquiryDataset.welcome],                         ["Hey, do you still have any of those Nike Air's?", InquiryDataset.dataset_search],                         ["Hi, what's the company that delivers your products?", InquiryDataset.delivery],                         ["Are those shoes good for winter", InquiryDataset.dataset_search],                         ["Have you got some budget smartphone? In range of 200-500 $", InquiryDataset.dataset_search],                         ["Are there any problems with delivery because of coronavirus? Isn't it going to take more "                          "time than usual?", InquiryDataset.delivery],                         ["Do you have a warranty for the new Nike sneakers?", InquiryDataset.dataset_search],                         ["When am I going to get the package?", InquiryDataset.delivery],                         ["Are the new IPhones 12 in stock?", InquiryDataset.dataset_search],                         ["Why does it take so long to get the package to my house?????", InquiryDataset.delivery],                         ["Hey, do you have something for a young girl. I need a gift for her birthday!", InquiryDataset.dataset_search],                         ["How much do you want for the refurbished iphone 6?", InquiryDataset.dataset_search],                         ["Hey, can you get us one pineapple and a chicago to 2411 Howard Street?", InquiryDataset.order],                         ["Hi man, how much time to get it to 2411 Howard Street?", InquiryDataset.delivery],                         ["Is that discount for pineapple pizza still available?", InquiryDataset.user_interaction_needed],                         ["Hey, do you work today?", InquiryDataset.welcome]])class InquiryAnalyzer(nn.Module):    """    Basically, neural network that classifies the inquiry embeddings based on the context.    """    # Initializing model here    def __init__(self, terminalUI=False):        # init the super of pytorch neural network        super(InquiryAnalyzer, self).__init__()        # Shrinking down the data        # Or in other words cutting down the data we don't need        if terminalUI:            self.epochScr = curses.initscr()        self.l1 = torch.nn.Linear(300, 100)        # self.l2 = torch.nn.Linear(100, 50)        self.lstm = torch.nn.LSTM(100, 20, 20)        self.softmax = torch.nn.Linear(20, 10)        self.float()    def forward(self, x, cellStateSize=1):        """        Forward propogation.        :param x: Tensor of words [1, 300]        :return: Tensor [?, 1, 10] with the inquiry classifications.        """        assert (isinstance(x, torch.nn.utils.rnn.PackedSequence))        # x here is the inquiry        cell_state = torch.zeros(20, cellStateSize, 20)  # check the correctness of the size        hidden_state = torch.zeros(20, cellStateSize, 20)  # check the correctness of the size        # Checking if all of the elements of array x(which is an inquiry basically)        result = None        # going through every word        if x.data[0].shape[0] != 300:            raise ValueError("The size of x has to be 300(vector features of the word)")        else:            currentInput = x            # (1) Densed layer(shrinking down)            currentInput = PackedSequenceHelper.squash_packed(currentInput, self.l1)            currentInput = PackedSequenceLeakyReluHelper.squash_packed_relu(currentInput, )            # currentInput = PackedSequenceHelper.squash_packed(currentInput)            # (2) Densed layer(shrinking down even more)            # currentInput = PackedSequenceHelper.squash_packed(currentInput, self.l2)            # currentInput = PackedSequenceHelper.squash_packed(currentInput)            # currentInput = PackedSequenceLeakyReluHelper.squash_packed_relu(currentInput)            # (3) LSTM Layer - based on the hidden state and cell state we predict what does the sentence mean            # In other words, what kind of inquiry user has made            (out, h0) = self.lstm(currentInput, (hidden_state, cell_state))            result = out            hidden_state = h0        # (4) - Softmax layer(output layer) | Classifying the inquiry        result = PackedSequenceHelper.squash_packed(result, self.softmax)        result = PackedSequenceHelper.squash_packed_softmax(result, dim=1)        return self._sequenceLabels(result)    def trainData(self, x, y, epochs=1000):        """        Trains the data over a sequence. Uses the MSE Loss.        :param x: Input data.        :param y: Labels used to estimate the loss over the dataset.        :param epochs: A number of epochs.        :return: List of loss values        """        assert (isinstance(x, torch.nn.utils.rnn.PackedSequence))        assert (isinstance(y, torch.Tensor))        # assert(isinstance(y, torch.Tensor))        # assert(isinstance(epochs, int))        # optimizer = optim.SGD(self.parameters(), lr=0.01, momentum=0.9)        optimizer = optim.Adam(self.parameters())        error = torch.nn.MSELoss()        losses = []        for i in range(epochs):            self._changeEpoch(i)            # batchSize = 0            # dataBatchStartIndex = 0            # for n in x.batch_sizes:            #     batchSize = n            #     dataBatchEnd = dataBatchStartIndex + batchSize            #     currentBatch = x.data[dataBatchStartIndex:dataBatchEnd]            output = self.forward(x.float(), cellStateSize=len(x.sorted_indices))            loss_value = error(output, y.float())            self._changeAccuracy(loss_value)            loss_value.backward()            optimizer.step()            losses.append(loss_value)        optimizer.zero_grad()        output = self.forward(x.float(), cellStateSize=len(x.sorted_indices))        loss_value = error(output, y.float())        self._changeAccuracy(loss_value)        loss_value.backward()        optimizer.step()        self.epochScr.erase()        print("Final result: {0}".format(output.float()))        print("\nLoss: {0}".format(loss_value))        self.epochScr.refresh()        plt.plot(losses)        plt.ylabel("losses")        plt.show()    def _sequenceLabels(self, packed):        """        Returns a Tensor of labels from a packed sequence.        :param packed:        :return:        """        temp = utils.pad_packed_sequence(packed, batch_first=True)        padded = temp[0]        lengths = temp[1]        result = torch.zeros(len(padded), 10)        for n, i in enumerate(padded):            curEl = padded[n, lengths[n] - 1]            result[n] = curEl        return result    def _changeEpoch(self, epoch):        """        Changes the epoch number to {epoch} in Terminal. Uses curses library.        :param epoch: Epoch number that the Terminal text has to changed to.        :return: None        """        self.epochScr.erase()        self.epochScr.addstr("Epoch: {0} \nAccuracy: ".format(epoch))        self.epochScr.refresh()        time.sleep(0.001)    def _changeAccuracy(self, accuracy):        """        Changes the accuracy to {accuracy} in Terminal. Uses curses library.        :param accuracy: Accuracy number.        :return: None        """        self.epochScr.addstr("{0}".format(accuracy))        self.epochScr.refresh()        time.sleep(0.001)class PackedSequenceHelper:    @staticmethod    def squash_packed(x, fn=torch.tanh):        """        Computes fn with an argument x.data, where x is PackedSequence.        :param x: PackedSequence that the function is processed with respect to its data.        :param fn: Function which we process.        :return: PackedSequence with the processed output of function fn with respect to x.data.        """        return torch.nn.utils.rnn.PackedSequence(fn(x.data), x.batch_sizes,                                                 x.sorted_indices, x.unsorted_indices)    @staticmethod    def squash_packed_softmax(x, dim=2):        """        Computes softmax with an argument x.data where x is PackedSequence        :param x: PackedSequence that the softmax is processed with respect to its data        :param dim: A dimension along which the softmax will be computed.        :return: A processed PackedSequence with softmax.        """        return torch.nn.utils.rnn.PackedSequence(F.softmax(x.data, dim), x.batch_sizes,                                                 x.sorted_indices, x.unsorted_indices)class PackedSequenceLeakyReluHelper:    @staticmethod    def squash_packed_relu(x, slope=0.1):        """        Squashes the PackedSequence with Leaky Relu Function.        :param x: The PackedSequence whose data we process with Leaky Relu.        :param slope: The slope number for softmax.        :return: Squashed PackedSequence.        """        return torch.nn.utils.rnn.PackedSequence(F.leaky_relu(x.data, negative_slope=slope), x.batch_sizes,                                                 x.sorted_indices, x.unsorted_indices)